{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Omegaraspati/omegaraspati/blob/master/balance_dataset_ep200_Adamax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzN4Ibet__rQ",
        "outputId": "10c64a85-9845-41c8-b5bb-d8e14a88df59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvtZaVQVADi9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications import vgg19\n",
        "# from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdthJf2wgvTN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXM-X16egxP9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr16RUs6AD1M"
      },
      "outputs": [],
      "source": [
        "train_path  = '/content/drive/MyDrive/Dataset/Train'\n",
        "valid_path  = '/content/drive/MyDrive/Dataset/Valid'\n",
        "test_path  = '/content/drive/MyDrive/Dataset/Test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI0TdwDiAE00"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "                             rescale=1./255,\n",
        "                             rotation_range=40,\n",
        "                             width_shift_range=0.2,\n",
        "                             height_shift_range=0.2,\n",
        "                             shear_range=0.2,\n",
        "                             zoom_range=0.2,\n",
        "                             horizontal_flip=True,\n",
        "                             fill_mode = 'nearest')\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkSGRF6oAFw3",
        "outputId": "39bc9801-1560-45ea-fcd8-4f345ec04c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3517 images belonging to 2 classes.\n",
            "Found 1005 images belonging to 2 classes.\n",
            "Found 490 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_datagen.flow_from_directory(train_path,\n",
        "                                              target_size=(224,224),\n",
        "                                              batch_size=16,\n",
        "                                              class_mode = 'categorical')\n",
        "\n",
        "valid_generator = val_datagen.flow_from_directory(valid_path,\n",
        "                                              target_size=(224,224),\n",
        "                                              batch_size=16,\n",
        "                                              class_mode = 'categorical')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(test_path,\n",
        "                                              target_size=(224,224),\n",
        "                                              batch_size=1,\n",
        "                                              class_mode = 'categorical',\n",
        "                                              shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_9YsZxEAGyg",
        "outputId": "65dc9ecc-a875-476c-d0d3-4b9bf697513d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Normal': 0, 'Osteoporosis': 1}\n"
          ]
        }
      ],
      "source": [
        "nama_class = train_generator.class_indices\n",
        "print(nama_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUUXvJX3AH3z",
        "outputId": "f53065f7-9cbc-4b30-913f-d52e81a0a21a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Normal', 'Osteoporosis']\n"
          ]
        }
      ],
      "source": [
        "li = list(nama_class.keys())\n",
        "print(li)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn4efiiUAJXV",
        "outputId": "31a3b2d1-71fd-4646-cdeb-7f266374c4bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "jumlah_class = len(nama_class)\n",
        "print(jumlah_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HQEaJkp4AKyT",
        "outputId": "72743f01-407c-48a4-ca28-653ccf5b1e6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 512)              0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4096)              2101248   \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 8194      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 38,915,138\n",
            "Trainable params: 38,654,978\n",
            "Non-trainable params: 260,160\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           Layer Type    Layer Name  \\\n",
              "0   <keras.engine.input_layer.InputLayer object at...       input_2   \n",
              "1   <keras.layers.convolutional.conv2d.Conv2D obje...  block1_conv1   \n",
              "2   <keras.layers.convolutional.conv2d.Conv2D obje...  block1_conv2   \n",
              "3   <keras.layers.pooling.max_pooling2d.MaxPooling...   block1_pool   \n",
              "4   <keras.layers.convolutional.conv2d.Conv2D obje...  block2_conv1   \n",
              "5   <keras.layers.convolutional.conv2d.Conv2D obje...  block2_conv2   \n",
              "6   <keras.layers.pooling.max_pooling2d.MaxPooling...   block2_pool   \n",
              "7   <keras.layers.convolutional.conv2d.Conv2D obje...  block3_conv1   \n",
              "8   <keras.layers.convolutional.conv2d.Conv2D obje...  block3_conv2   \n",
              "9   <keras.layers.convolutional.conv2d.Conv2D obje...  block3_conv3   \n",
              "10  <keras.layers.convolutional.conv2d.Conv2D obje...  block3_conv4   \n",
              "11  <keras.layers.pooling.max_pooling2d.MaxPooling...   block3_pool   \n",
              "12  <keras.layers.convolutional.conv2d.Conv2D obje...  block4_conv1   \n",
              "13  <keras.layers.convolutional.conv2d.Conv2D obje...  block4_conv2   \n",
              "14  <keras.layers.convolutional.conv2d.Conv2D obje...  block4_conv3   \n",
              "15  <keras.layers.convolutional.conv2d.Conv2D obje...  block4_conv4   \n",
              "16  <keras.layers.pooling.max_pooling2d.MaxPooling...   block4_pool   \n",
              "17  <keras.layers.convolutional.conv2d.Conv2D obje...  block5_conv1   \n",
              "18  <keras.layers.convolutional.conv2d.Conv2D obje...  block5_conv2   \n",
              "19  <keras.layers.convolutional.conv2d.Conv2D obje...  block5_conv3   \n",
              "20  <keras.layers.convolutional.conv2d.Conv2D obje...  block5_conv4   \n",
              "21  <keras.layers.pooling.max_pooling2d.MaxPooling...   block5_pool   \n",
              "\n",
              "    Layer Trainable  \n",
              "0             False  \n",
              "1             False  \n",
              "2             False  \n",
              "3             False  \n",
              "4             False  \n",
              "5             False  \n",
              "6             False  \n",
              "7              True  \n",
              "8              True  \n",
              "9              True  \n",
              "10             True  \n",
              "11             True  \n",
              "12             True  \n",
              "13             True  \n",
              "14             True  \n",
              "15             True  \n",
              "16             True  \n",
              "17             True  \n",
              "18             True  \n",
              "19             True  \n",
              "20             True  \n",
              "21             True  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-129c72b7-5e33-45aa-b08d-db2a6c9a7cbb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Layer Type</th>\n",
              "      <th>Layer Name</th>\n",
              "      <th>Layer Trainable</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;keras.engine.input_layer.InputLayer object at...</td>\n",
              "      <td>input_2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block1_conv1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block1_conv2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;keras.layers.pooling.max_pooling2d.MaxPooling...</td>\n",
              "      <td>block1_pool</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block2_conv1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block2_conv2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>&lt;keras.layers.pooling.max_pooling2d.MaxPooling...</td>\n",
              "      <td>block2_pool</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block3_conv1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block3_conv2</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block3_conv3</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block3_conv4</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>&lt;keras.layers.pooling.max_pooling2d.MaxPooling...</td>\n",
              "      <td>block3_pool</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block4_conv1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block4_conv2</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block4_conv3</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block4_conv4</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>&lt;keras.layers.pooling.max_pooling2d.MaxPooling...</td>\n",
              "      <td>block4_pool</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block5_conv1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block5_conv2</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block5_conv3</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>&lt;keras.layers.convolutional.conv2d.Conv2D obje...</td>\n",
              "      <td>block5_conv4</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>&lt;keras.layers.pooling.max_pooling2d.MaxPooling...</td>\n",
              "      <td>block5_pool</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-129c72b7-5e33-45aa-b08d-db2a6c9a7cbb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-129c72b7-5e33-45aa-b08d-db2a6c9a7cbb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-129c72b7-5e33-45aa-b08d-db2a6c9a7cbb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "IMG_WIDTH=224\n",
        "IMG_HEIGHT=224\n",
        "IMG_DIM = (IMG_WIDTH, IMG_HEIGHT, 3)\n",
        "\n",
        "vgg_model = vgg19.VGG19(weights = \"imagenet\", include_top=False, input_shape = IMG_DIM)\n",
        "\n",
        "#inisialisasi layer terakhir pada block5 sbg output\n",
        "for layer in vgg_model.layers[:-15]:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = vgg_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(units=4096, activation='relu')(x)\n",
        "x = Dense(units=4096, activation='relu')(x)\n",
        "output  = Dense(units=2, activation='softmax')(x)\n",
        "model = Model(vgg_model.input, output)\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "layers = [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]\n",
        "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjKyRljQ-Fm3",
        "outputId": "d40db3f2-32d3-4b9b-f0c8-5be0d4be90e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adamax.py:95: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adamax, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adamax(lr=0.0001),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM0pkIZ6ATEy",
        "outputId": "b37f93ef-87c1-44a2-d19a-cc7941a6d0e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-384d4da66c72>:16: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(train_generator, steps_per_epoch=step_size_train,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.6453 - accuracy: 0.6424\n",
            "Epoch 1: val_accuracy improved from -inf to 0.61996, saving model to /content/drive/MyDrive/Hasil Training/Adamax200.hdf5\n",
            "219/219 [==============================] - 1629s 7s/step - loss: 0.6453 - accuracy: 0.6424 - val_loss: 0.7278 - val_accuracy: 0.6200\n",
            "Epoch 2/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5638 - accuracy: 0.7372\n",
            "Epoch 2: val_accuracy improved from 0.61996 to 0.65827, saving model to /content/drive/MyDrive/Hasil Training/Adamax200.hdf5\n",
            "219/219 [==============================] - 80s 364ms/step - loss: 0.5638 - accuracy: 0.7372 - val_loss: 0.6352 - val_accuracy: 0.6583\n",
            "Epoch 3/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5332 - accuracy: 0.7498\n",
            "Epoch 3: val_accuracy improved from 0.65827 to 0.68851, saving model to /content/drive/MyDrive/Hasil Training/Adamax200.hdf5\n",
            "219/219 [==============================] - 78s 355ms/step - loss: 0.5332 - accuracy: 0.7498 - val_loss: 0.6042 - val_accuracy: 0.6885\n",
            "Epoch 4/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5101 - accuracy: 0.7701\n",
            "Epoch 4: val_accuracy improved from 0.68851 to 0.69456, saving model to /content/drive/MyDrive/Hasil Training/Adamax200.hdf5\n",
            "219/219 [==============================] - 78s 357ms/step - loss: 0.5101 - accuracy: 0.7701 - val_loss: 0.7166 - val_accuracy: 0.6946\n",
            "Epoch 5/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4946 - accuracy: 0.7698\n",
            "Epoch 5: val_accuracy did not improve from 0.69456\n",
            "219/219 [==============================] - 74s 339ms/step - loss: 0.4946 - accuracy: 0.7698 - val_loss: 0.5210 - val_accuracy: 0.6885\n",
            "Epoch 6/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4409 - accuracy: 0.8052\n",
            "Epoch 6: val_accuracy improved from 0.69456 to 0.80645, saving model to /content/drive/MyDrive/Hasil Training/Adamax200.hdf5\n",
            "219/219 [==============================] - 79s 362ms/step - loss: 0.4409 - accuracy: 0.8052 - val_loss: 0.4415 - val_accuracy: 0.8065\n",
            "Epoch 7/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4300 - accuracy: 0.8092\n",
            "Epoch 7: val_accuracy did not improve from 0.80645\n",
            "219/219 [==============================] - 76s 349ms/step - loss: 0.4300 - accuracy: 0.8092 - val_loss: 0.4853 - val_accuracy: 0.7540\n",
            "Epoch 8/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3908 - accuracy: 0.8369\n",
            "Epoch 8: val_accuracy did not improve from 0.80645\n",
            "219/219 [==============================] - 74s 337ms/step - loss: 0.3908 - accuracy: 0.8369 - val_loss: 0.5353 - val_accuracy: 0.7389\n",
            "Epoch 9/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3390 - accuracy: 0.8612\n",
            "Epoch 9: val_accuracy improved from 0.80645 to 0.84073, saving model to /content/drive/MyDrive/Hasil Training/Adamax200.hdf5\n",
            "219/219 [==============================] - 79s 362ms/step - loss: 0.3390 - accuracy: 0.8612 - val_loss: 0.3511 - val_accuracy: 0.8407\n",
            "Epoch 10/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3053 - accuracy: 0.8726\n",
            "Epoch 10: val_accuracy improved from 0.84073 to 0.84778, saving model to /content/drive/MyDrive/Hasil Training/Adamax200.hdf5\n",
            "219/219 [==============================] - 80s 363ms/step - loss: 0.3053 - accuracy: 0.8726 - val_loss: 0.3708 - val_accuracy: 0.8478\n",
            "Epoch 11/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.8880\n",
            "Epoch 11: val_accuracy improved from 0.84778 to 0.90323, saving model to /content/drive/MyDrive/Hasil Training/Adamax200.hdf5\n",
            "219/219 [==============================] - 77s 353ms/step - loss: 0.2677 - accuracy: 0.8880 - val_loss: 0.2184 - val_accuracy: 0.9032\n",
            "Epoch 12/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.2312 - accuracy: 0.9020\n",
            "Epoch 12: val_accuracy improved from 0.90323 to 0.92440, saving model to /content/drive/MyDrive/Hasil Training/Adamax200.hdf5\n",
            "219/219 [==============================] - 76s 347ms/step - loss: 0.2312 - accuracy: 0.9020 - val_loss: 0.1847 - val_accuracy: 0.9244\n",
            "Epoch 13/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 0.9217\n",
            "Epoch 13: val_accuracy did not improve from 0.92440\n",
            "219/219 [==============================] - 76s 349ms/step - loss: 0.1940 - accuracy: 0.9217 - val_loss: 0.2704 - val_accuracy: 0.9062\n",
            "Epoch 14/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9306\n",
            "Epoch 14: val_accuracy improved from 0.92440 to 0.93548, saving model to /content/drive/MyDrive/Hasil Training/Adamax200.hdf5\n",
            "219/219 [==============================] - 77s 350ms/step - loss: 0.1669 - accuracy: 0.9306 - val_loss: 0.1639 - val_accuracy: 0.9355\n",
            "Epoch 15/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1513 - accuracy: 0.9386\n",
            "Epoch 15: val_accuracy improved from 0.93548 to 0.93851, saving model to /content/drive/MyDrive/Hasil Training/Adamax200.hdf5\n",
            "219/219 [==============================] - 77s 352ms/step - loss: 0.1513 - accuracy: 0.9386 - val_loss: 0.1886 - val_accuracy: 0.9385\n",
            "Epoch 16/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9440\n",
            "Epoch 16: val_accuracy improved from 0.93851 to 0.95565, saving model to /content/drive/MyDrive/Hasil Training/Adamax200.hdf5\n",
            "219/219 [==============================] - 78s 355ms/step - loss: 0.1399 - accuracy: 0.9440 - val_loss: 0.1544 - val_accuracy: 0.9556\n",
            "Epoch 17/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9432\n",
            "Epoch 17: val_accuracy did not improve from 0.95565\n",
            "219/219 [==============================] - 73s 335ms/step - loss: 0.1452 - accuracy: 0.9432 - val_loss: 0.4158 - val_accuracy: 0.8790\n",
            "Epoch 18/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9466\n",
            "Epoch 18: val_accuracy did not improve from 0.95565\n",
            "219/219 [==============================] - 75s 342ms/step - loss: 0.1299 - accuracy: 0.9466 - val_loss: 0.1525 - val_accuracy: 0.9425\n",
            "Epoch 19/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1129 - accuracy: 0.9512\n",
            "Epoch 19: val_accuracy did not improve from 0.95565\n",
            "219/219 [==============================] - 67s 305ms/step - loss: 0.1129 - accuracy: 0.9512 - val_loss: 0.2724 - val_accuracy: 0.9284\n",
            "Epoch 20/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9500\n",
            "Epoch 20: val_accuracy did not improve from 0.95565\n",
            "219/219 [==============================] - 64s 292ms/step - loss: 0.1194 - accuracy: 0.9500 - val_loss: 0.1305 - val_accuracy: 0.9466\n",
            "Epoch 21/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1055 - accuracy: 0.9546\n",
            "Epoch 21: val_accuracy improved from 0.95565 to 0.96270, saving model to /content/drive/MyDrive/Hasil Training/Adamax200.hdf5\n",
            "219/219 [==============================] - 68s 310ms/step - loss: 0.1055 - accuracy: 0.9546 - val_loss: 0.1086 - val_accuracy: 0.9627\n",
            "Epoch 22/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0897 - accuracy: 0.9612\n",
            "Epoch 22: val_accuracy did not improve from 0.96270\n",
            "219/219 [==============================] - 69s 312ms/step - loss: 0.0897 - accuracy: 0.9612 - val_loss: 0.1638 - val_accuracy: 0.9617\n",
            "Epoch 23/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1056 - accuracy: 0.9546\n",
            "Epoch 23: val_accuracy did not improve from 0.96270\n",
            "219/219 [==============================] - 67s 304ms/step - loss: 0.1056 - accuracy: 0.9546 - val_loss: 0.2149 - val_accuracy: 0.9335\n",
            "Epoch 24/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9617\n",
            "Epoch 24: val_accuracy improved from 0.96270 to 0.96573, saving model to /content/drive/MyDrive/Hasil Training/Adamax200.hdf5\n",
            "219/219 [==============================] - 68s 311ms/step - loss: 0.0944 - accuracy: 0.9617 - val_loss: 0.0998 - val_accuracy: 0.9657\n",
            "Epoch 25/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9614\n",
            "Epoch 25: val_accuracy did not improve from 0.96573\n",
            "219/219 [==============================] - 68s 310ms/step - loss: 0.0889 - accuracy: 0.9614 - val_loss: 0.1702 - val_accuracy: 0.9607\n",
            "Epoch 26/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9603\n",
            "Epoch 26: val_accuracy did not improve from 0.96573\n",
            "219/219 [==============================] - 65s 294ms/step - loss: 0.0868 - accuracy: 0.9603 - val_loss: 0.2753 - val_accuracy: 0.9435\n",
            "Epoch 27/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9652\n",
            "Epoch 27: val_accuracy did not improve from 0.96573\n",
            "219/219 [==============================] - 68s 309ms/step - loss: 0.0748 - accuracy: 0.9652 - val_loss: 0.2588 - val_accuracy: 0.9556\n",
            "Epoch 28/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9617\n",
            "Epoch 28: val_accuracy did not improve from 0.96573\n",
            "219/219 [==============================] - 68s 310ms/step - loss: 0.0857 - accuracy: 0.9617 - val_loss: 0.2504 - val_accuracy: 0.9446\n",
            "Epoch 29/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9654\n",
            "Epoch 29: val_accuracy did not improve from 0.96573\n",
            "219/219 [==============================] - 69s 313ms/step - loss: 0.0822 - accuracy: 0.9654 - val_loss: 0.1440 - val_accuracy: 0.9466\n",
            "Epoch 30/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9654\n",
            "Epoch 30: val_accuracy improved from 0.96573 to 0.98286, saving model to /content/drive/MyDrive/Hasil Training/Adamax200.hdf5\n",
            "219/219 [==============================] - 70s 317ms/step - loss: 0.0775 - accuracy: 0.9654 - val_loss: 0.0660 - val_accuracy: 0.9829\n",
            "Epoch 31/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0854 - accuracy: 0.9614\n",
            "Epoch 31: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 66s 299ms/step - loss: 0.0854 - accuracy: 0.9614 - val_loss: 0.1337 - val_accuracy: 0.9486\n",
            "Epoch 32/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9694\n",
            "Epoch 32: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 69s 314ms/step - loss: 0.0669 - accuracy: 0.9694 - val_loss: 0.0874 - val_accuracy: 0.9647\n",
            "Epoch 33/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9652\n",
            "Epoch 33: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 69s 313ms/step - loss: 0.0825 - accuracy: 0.9652 - val_loss: 0.6713 - val_accuracy: 0.8639\n",
            "Epoch 34/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9674\n",
            "Epoch 34: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 65s 298ms/step - loss: 0.0755 - accuracy: 0.9674 - val_loss: 0.1558 - val_accuracy: 0.9466\n",
            "Epoch 35/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9563\n",
            "Epoch 35: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 65s 298ms/step - loss: 0.0878 - accuracy: 0.9563 - val_loss: 0.1654 - val_accuracy: 0.9556\n",
            "Epoch 36/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9700\n",
            "Epoch 36: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 69s 313ms/step - loss: 0.0654 - accuracy: 0.9700 - val_loss: 0.1621 - val_accuracy: 0.9536\n",
            "Epoch 37/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9654\n",
            "Epoch 37: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 65s 298ms/step - loss: 0.0718 - accuracy: 0.9654 - val_loss: 0.2428 - val_accuracy: 0.9325\n",
            "Epoch 38/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9686\n",
            "Epoch 38: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 69s 314ms/step - loss: 0.0711 - accuracy: 0.9686 - val_loss: 0.1722 - val_accuracy: 0.9577\n",
            "Epoch 39/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9683\n",
            "Epoch 39: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 69s 314ms/step - loss: 0.0650 - accuracy: 0.9683 - val_loss: 0.2104 - val_accuracy: 0.9496\n",
            "Epoch 40/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9657\n",
            "Epoch 40: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 65s 297ms/step - loss: 0.0716 - accuracy: 0.9657 - val_loss: 0.0999 - val_accuracy: 0.9657\n",
            "Epoch 41/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9654\n",
            "Epoch 41: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 66s 301ms/step - loss: 0.0726 - accuracy: 0.9654 - val_loss: 0.1782 - val_accuracy: 0.9556\n",
            "Epoch 42/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9694\n",
            "Epoch 42: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 65s 298ms/step - loss: 0.0636 - accuracy: 0.9694 - val_loss: 0.2674 - val_accuracy: 0.9526\n",
            "Epoch 43/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9697\n",
            "Epoch 43: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 65s 294ms/step - loss: 0.0608 - accuracy: 0.9697 - val_loss: 0.1541 - val_accuracy: 0.9597\n",
            "Epoch 44/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9717\n",
            "Epoch 44: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 67s 307ms/step - loss: 0.0657 - accuracy: 0.9717 - val_loss: 0.2311 - val_accuracy: 0.9294\n",
            "Epoch 45/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9694\n",
            "Epoch 45: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 292ms/step - loss: 0.0648 - accuracy: 0.9694 - val_loss: 0.2195 - val_accuracy: 0.9567\n",
            "Epoch 46/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9723\n",
            "Epoch 46: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 293ms/step - loss: 0.0616 - accuracy: 0.9723 - val_loss: 0.1864 - val_accuracy: 0.9415\n",
            "Epoch 47/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9689\n",
            "Epoch 47: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 65s 294ms/step - loss: 0.0593 - accuracy: 0.9689 - val_loss: 0.3132 - val_accuracy: 0.9274\n",
            "Epoch 48/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9646\n",
            "Epoch 48: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 293ms/step - loss: 0.0794 - accuracy: 0.9646 - val_loss: 0.1254 - val_accuracy: 0.9637\n",
            "Epoch 49/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9703\n",
            "Epoch 49: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 292ms/step - loss: 0.0603 - accuracy: 0.9703 - val_loss: 0.1273 - val_accuracy: 0.9617\n",
            "Epoch 50/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9697\n",
            "Epoch 50: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 290ms/step - loss: 0.0538 - accuracy: 0.9697 - val_loss: 0.1796 - val_accuracy: 0.9486\n",
            "Epoch 51/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9743\n",
            "Epoch 51: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 290ms/step - loss: 0.0541 - accuracy: 0.9743 - val_loss: 0.1286 - val_accuracy: 0.9677\n",
            "Epoch 52/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9686\n",
            "Epoch 52: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 289ms/step - loss: 0.0662 - accuracy: 0.9686 - val_loss: 0.1846 - val_accuracy: 0.9425\n",
            "Epoch 53/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0540 - accuracy: 0.9720\n",
            "Epoch 53: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 63s 286ms/step - loss: 0.0540 - accuracy: 0.9720 - val_loss: 0.2883 - val_accuracy: 0.9415\n",
            "Epoch 54/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9749\n",
            "Epoch 54: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 63s 289ms/step - loss: 0.0573 - accuracy: 0.9749 - val_loss: 0.1313 - val_accuracy: 0.9667\n",
            "Epoch 55/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9717\n",
            "Epoch 55: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 290ms/step - loss: 0.0550 - accuracy: 0.9717 - val_loss: 0.1431 - val_accuracy: 0.9657\n",
            "Epoch 56/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9697\n",
            "Epoch 56: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 67s 307ms/step - loss: 0.0596 - accuracy: 0.9697 - val_loss: 0.1604 - val_accuracy: 0.9587\n",
            "Epoch 57/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9712\n",
            "Epoch 57: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 292ms/step - loss: 0.0566 - accuracy: 0.9712 - val_loss: 0.2746 - val_accuracy: 0.9476\n",
            "Epoch 58/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9732\n",
            "Epoch 58: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 68s 312ms/step - loss: 0.0529 - accuracy: 0.9732 - val_loss: 0.1783 - val_accuracy: 0.9667\n",
            "Epoch 59/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9657\n",
            "Epoch 59: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 293ms/step - loss: 0.0748 - accuracy: 0.9657 - val_loss: 0.1397 - val_accuracy: 0.9667\n",
            "Epoch 60/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9774\n",
            "Epoch 60: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 290ms/step - loss: 0.0511 - accuracy: 0.9774 - val_loss: 0.1597 - val_accuracy: 0.9627\n",
            "Epoch 61/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9723\n",
            "Epoch 61: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 67s 307ms/step - loss: 0.0550 - accuracy: 0.9723 - val_loss: 0.1733 - val_accuracy: 0.9546\n",
            "Epoch 62/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9729\n",
            "Epoch 62: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 290ms/step - loss: 0.0526 - accuracy: 0.9729 - val_loss: 0.2250 - val_accuracy: 0.9627\n",
            "Epoch 63/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9712\n",
            "Epoch 63: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 67s 304ms/step - loss: 0.0577 - accuracy: 0.9712 - val_loss: 0.1196 - val_accuracy: 0.9667\n",
            "Epoch 64/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9712\n",
            "Epoch 64: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 290ms/step - loss: 0.0598 - accuracy: 0.9712 - val_loss: 0.4017 - val_accuracy: 0.9304\n",
            "Epoch 65/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9700\n",
            "Epoch 65: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 291ms/step - loss: 0.0627 - accuracy: 0.9700 - val_loss: 0.1814 - val_accuracy: 0.9546\n",
            "Epoch 66/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9703\n",
            "Epoch 66: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 290ms/step - loss: 0.0582 - accuracy: 0.9703 - val_loss: 0.2535 - val_accuracy: 0.9476\n",
            "Epoch 67/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9751\n",
            "Epoch 67: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 293ms/step - loss: 0.0533 - accuracy: 0.9751 - val_loss: 0.1725 - val_accuracy: 0.9617\n",
            "Epoch 68/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9751\n",
            "Epoch 68: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 291ms/step - loss: 0.0524 - accuracy: 0.9751 - val_loss: 0.1393 - val_accuracy: 0.9677\n",
            "Epoch 69/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.9743\n",
            "Epoch 69: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 64s 290ms/step - loss: 0.0552 - accuracy: 0.9743 - val_loss: 0.2603 - val_accuracy: 0.9567\n",
            "Epoch 70/200\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9757\n",
            "Epoch 70: val_accuracy did not improve from 0.98286\n",
            "219/219 [==============================] - 65s 294ms/step - loss: 0.0510 - accuracy: 0.9757 - val_loss: 0.0976 - val_accuracy: 0.9718\n",
            "Epoch 71/200\n",
            " 17/219 [=>............................] - ETA: 55s - loss: 0.0427 - accuracy: 0.9779"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, TensorBoard\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "now = datetime.datetime.now\n",
        "t = now()\n",
        "\n",
        "NAME = \"logs{}\".format(int(time.time()))\n",
        "tensorboard = TensorBoard(log_dir='/content/drive/MyDrive/Hasil Training/{}'.format(NAME))\n",
        "checkpointer = ModelCheckpoint(filepath='/content/drive/MyDrive/Hasil Training/Adamax200.hdf5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "step_size_train = train_generator.n//train_generator.batch_size\n",
        "step_size_valid = valid_generator.n//valid_generator.batch_size\n",
        "\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=step_size_train,\n",
        "                   validation_data=valid_generator, validation_steps=step_size_valid,\n",
        "                   epochs=200,\n",
        "                   verbose=1,\n",
        "                   callbacks=[checkpointer, tensorboard])\n",
        "\n",
        "print('Training time: %s' % (now() - t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh3AF3kpeher"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez6Se6qnehbW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRxuECEzAhy0"
      },
      "outputs": [],
      "source": [
        "# Retrieve a list of accuracy results on training and test data\n",
        "# sets for each training epoch\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "# Plot training and validation accuracy per epoch\n",
        "plt.plot(epochs, acc, label = \"training\")\n",
        "plt.plot(epochs, val_acc, label = \"validation\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.grid(False)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and validation accuracy VGG19')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Plot training and validation loss per epoch\n",
        "plt.plot(epochs, loss, label = \"training\")\n",
        "plt.plot(epochs, val_loss, label = \"validation\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.grid(False)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and validation loss VGG19')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwKz5dDMAjGe"
      },
      "outputs": [],
      "source": [
        "def save_model_result(model):\n",
        "\t'''\n",
        "\t\t-- model : compiled model on the given data.\n",
        "\t'''\n",
        "\n",
        "\t# Extract model result data to separate data vectors\n",
        "\tdata_val_acc = list(model.history.history['val_accuracy'])\n",
        "\tdata_acc = list(model.history.history['accuracy'])\n",
        "\tdata_val_loss = list(model.history.history['val_loss'])\n",
        "\tdata_loss = list(model.history.history['loss'])\n",
        "\n",
        "\t# Convert Model result data to dataframe by using dictionary dat structure\n",
        "\td = {}\n",
        "\td['val_accuracy'] = data_val_acc\n",
        "\td['accuracy'] = data_acc\n",
        "\td['val_loss'] = data_val_loss\n",
        "\td['loss'] = data_loss\n",
        "\n",
        "\tdf = pd.DataFrame(d)\n",
        "\tprint(df)\n",
        "\n",
        "\t# Get a current timestamp\n",
        "\ttimestamp = str(datetime.datetime.now()).replace(\":\",\"-\")[:-10].replace(' ', '_')\n",
        "\tfilename = '/content/drive/MyDrive/Hasil Training/Adamax200{}.csv'.format(timestamp)\n",
        "\tdf.to_csv(filename, encoding='utf-8')\n",
        "\n",
        "\tprint('\\n\\nResult data is saved as file: {}'.format(filename))\n",
        "\n",
        "\treturn df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRVfO_MaAmeA"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "\n",
        "model = model\n",
        "df = save_model_result(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf5cZoxHAy9z"
      },
      "outputs": [],
      "source": [
        "test_dir = '/content/drive/MyDrive/Dataset/Test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqEE5UoJA8-d"
      },
      "outputs": [],
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1YODzKPA_p9"
      },
      "outputs": [],
      "source": [
        "test_generator = test_datagen.flow_from_directory(directory=test_dir,\n",
        "                                                  target_size=(224,224),\n",
        "                                                  batch_size=1,\n",
        "                                                  class_mode='categorical',\n",
        "                                                  shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74hoMgtWBDMu"
      },
      "outputs": [],
      "source": [
        "nama_class = test_generator.class_indices\n",
        "print(nama_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rusrY1FBFp2"
      },
      "outputs": [],
      "source": [
        "li = list(nama_class.keys())\n",
        "print(li)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PT8thpVBI4-"
      },
      "outputs": [],
      "source": [
        "jum_classes = test_generator.num_classes\n",
        "jum_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otxTN61CBKoi"
      },
      "outputs": [],
      "source": [
        "steps_test = test_generator.n / test_generator.batch_size\n",
        "steps_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt_zkQCGBOEG"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_files\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "\n",
        "def load_dataset(path):\n",
        "    data = load_files(path)\n",
        "    paths = np.array(data['filenames'])\n",
        "    targets = np_utils.to_categorical(np.array(data['target']))\n",
        "    return paths, targets\n",
        "\n",
        "test_files, test_targets = load_dataset('/content/drive/MyDrive/Dataset/Test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqEQ4XnNBT2U"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "# from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def path_to_tensor(img_path):\n",
        "    #load image, resize ke ukuran 224,224\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    #convert gambar jadi 3D tensor (224,224,3)\n",
        "    x = image.img_to_array(img).astype(np.float32)/255\n",
        "    # x = image.img_to_array(img)\n",
        "    return np.expand_dims(x, axis=0)\n",
        "\n",
        "def paths_to_tensor(img_paths):\n",
        "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
        "    return np.vstack(list_of_tensors)\n",
        "\n",
        "test_tensors = (paths_to_tensor(test_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d47vGZ2SBWS8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/drive/MyDrive/Hasil Training/Adamax200.hdf5')\n",
        "result = print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*model.evaluate(test_tensors, test_targets, batch_size=test_generator.batch_size)))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "cm_labels = ['Normal','Osteoporosis']\n",
        "\n",
        "cm = confusion_matrix(np.argmax(test_targets, axis=1),\n",
        "                      np.argmax(model.predict(test_tensors), axis=1))\n",
        "plt.imshow(cm, cmap=plt.cm.Blues)\n",
        "plt.colorbar()\n",
        "indexes = np.arange(len(cm_labels))\n",
        "for i in indexes:\n",
        "    for j in indexes:\n",
        "        plt.text(j, i, cm[i, j])\n",
        "plt.xticks(indexes, cm_labels, rotation=90)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.yticks(indexes, cm_labels)\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion matrix')\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-L1-sjhABYyM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "test_dir = '/content/drive/MyDrive/Dataset/Test'\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size= 1,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "\n",
        "model_vgg19 = load_model('/content/drive/MyDrive/Hasil Training/Adamax200.hdf5')\n",
        "\n",
        "#Confusion Matrix and Classification Report\n",
        "Y_pred_vgg19 = model_vgg19.predict_generator(test_generator, test_generator.samples // test_generator.batch_size)\n",
        "y_pred_vgg19 = np.argmax(Y_pred_vgg19, axis=1)\n",
        "\n",
        "\n",
        "print('Confusion Matrix VGG19')\n",
        "print(confusion_matrix(test_generator.classes, y_pred_vgg19))\n",
        "print('Classification Report')\n",
        "target_names = ['Normal','Osteoporosis']\n",
        "print(classification_report(test_generator.classes, y_pred_vgg19, target_names=target_names))\n",
        "\n",
        "#Evaluating model:\n",
        "x, y = zip(*(test_generator[i] for i in range(len(test_generator))))\n",
        "x_test, y_test = np.vstack(x), np.vstack(y)\n",
        "loss_vgg19, acc_vgg19 = model_vgg19.evaluate(x_test, y_test, batch_size=32)\n",
        "\n",
        "print(\"Accuracy: \" ,acc_vgg19)\n",
        "print(\"Loss: \", loss_vgg19)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiwNJ9apGtNy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import lite\n",
        "\n",
        "\n",
        "vgg19 = tf.keras.models.load_model('/content/drive/MyDrive/Hasil Training/Adamax200.hdf5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(vgg19)\n",
        "converter.experimental_new_converter = True\n",
        "vgg19 = converter.convert()\n",
        "file = open( '/content/drive/MyDrive/Hasil Training/Adamax200.tflite' , 'wb' )\n",
        "file.write( vgg19 )\n",
        "print(\" vgg19 berhasil di convert\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}